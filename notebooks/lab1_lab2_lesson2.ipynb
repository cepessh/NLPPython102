{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1 + Lab 2 Lesson 2: BPE, Zipf slope, and n-gram basics\n",
        "\n",
        "**What we use**\n",
        "- `datasets` (Hugging Face) for loading and splitting datasets.\n",
        "- `transformers` (Hugging Face) for BPE tokenization via `AutoTokenizer`.\n",
        "- `numpy` for log-log slope fitting and random sampling utilities.\n",
        "- `Counter` for frequency counts.\n",
        "\n",
        "**Goals**\n",
        "- Apply BPE tokenization and inspect subword behavior.\n",
        "- Fit a Zipf slope on log-log axes and interpret it.\n",
        "- Start Lab 2: create train/dev/test splits and handle unknown tokens.\n",
        "- Build n-gram counts, add smoothing, and compute perplexity.\n",
        "- Generate short samples with top-k sampling.\n",
        "\n",
        "**Structure**\n",
        "1) Load a dataset subset.\n",
        "2) BPE tokenization practice.\n",
        "3) Zipf slope fitting.\n",
        "4) Lab 2 intro: split + UNK handling.\n",
        "5) N-gram counts + add-alpha smoothing.\n",
        "6) Perplexity + simple grid search.\n",
        "7) Top-k sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why the fixed seed matters here**\n",
        "- It makes `train_test_split` reproducible when we create train/dev/test.\n",
        "- It stabilizes `np.random.choice` in top-k sampling so examples are repeatable.\n",
        "- It does not change the dataset content itself, only the randomized operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load a dataset subset\n",
        "Choose a dataset and a text field. Keep a small subset for speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset choice\n",
        "# Examples: name = 'ag_news' (text), 'imdb' (text), 'yelp_polarity' (text)\n",
        "name = 'ag_news'\n",
        "config = None\n",
        "text_field = 'text'\n",
        "\n",
        "if config:\n",
        "    ds = load_dataset(name, config, split='train')\n",
        "else:\n",
        "    ds = load_dataset(name, split='train')\n",
        "\n",
        "subset = ds.select(range(2000))\n",
        "print(subset.features)\n",
        "print(subset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print('---')\n",
        "    print(subset[i][text_field])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = [ex[text_field] for ex in subset if ex[text_field].strip()]\n",
        "print('sample texts:', len(texts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: BPE tokenization\n",
        "We use a pretrained BPE tokenizer to see how subwords split words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About `AutoTokenizer` and BPE**\n",
        "- `AutoTokenizer` loads a pretrained tokenizer from the Hugging Face Hub.\n",
        "- For GPT-2, this is a Byte-Pair Encoding (BPE) tokenizer that splits words into subwords.\n",
        "- `tokenizer(text)` returns `input_ids` (integers), and `convert_ids_to_tokens` shows subword pieces.\n",
        "- We set `add_special_tokens=False` to keep the raw segmentation visible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What is BPE?**\n",
        "Byte-Pair Encoding (BPE) builds a subword vocabulary by repeatedly merging\n",
        "the most frequent *adjacent* symbol pairs. Symbols start as characters and\n",
        "become longer subwords after merges.\n",
        "\n",
        "BPE is an iterative process: after each merge, pairs are re-counted and the next most frequent pair is merged.\n",
        "\n",
        "Example (simplified):\n",
        "$$t\\ h\\ e\\ r\\ e \\xrightarrow{(t,h)} th\\ e\\ r\\ e \\xrightarrow{(th,e)} the\\ r\\ e$$\n",
        "Here `(t,h)` is chosen because it is the most frequent adjacent pair in the corpus at that step;\n",
        "then `(th,e)` becomes frequent and is merged next. That is why the tokens become `the | r | e`.\n",
        "\n",
        "**Can we compute Zipf on BPE tokens?** Yes. If you count BPE tokens instead of words,\n",
        "you can make a Zipf plot for subword units. The curve shape can change because the vocabulary\n",
        "now includes short subword pieces.\n",
        "\n",
        "**Where BPE is used**\n",
        "BPE tokenization is the standard input step for many pretrained transformer models.\n",
        "In later labs, we will feed BPE token IDs into models and compare tokenization effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: pick 3 sentences and inspect BPE tokens\n",
        "# Hint: enc = tokenizer(text, add_special_tokens=False)\n",
        "# Hint: tokenizer.convert_ids_to_tokens(enc['input_ids'])\n",
        "# Hint: tokenizer.decode(enc['input_ids'])\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Zipf slope fitting\n",
        "We fit a line to log(rank) vs log(freq) for ranks 10-100 to estimate the slope.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About Zipf slope fitting**\n",
        "- We compute token frequencies from a simple whitespace tokenizer.\n",
        "- On a log-log plot, Zipf-like data forms an approximate straight line.\n",
        "- We fit only a middle rank range (start/end) to avoid very frequent function words\n",
        "  at the head and sparse, noisy counts in the tail.\n",
        "- The slope is negative; a less steep slope often suggests higher lexical diversity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**How `np.polyfit` works here**\n",
        "- `np.polyfit(x, y, 1)` fits a line `y = m*x + b` by least squares.\n",
        "- It returns `[m, b]` where `m` is the slope and `b` is the intercept.\n",
        "- On log-log data, the slope estimates the Zipf exponent.\n",
        "- We fit only a middle rank range to reduce head/tail distortion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_whitespace(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def get_token_counts(texts):\n",
        "    counts = Counter()\n",
        "    for t in texts:\n",
        "        counts.update(tokenize_whitespace(t))\n",
        "    return counts\n",
        "\n",
        "texts = [ex[text_field] for ex in subset if ex[text_field].strip()]\n",
        "counts = get_token_counts(texts)\n",
        "freqs = sorted(counts.values(), reverse=True)\n",
        "ranks = np.arange(1, len(freqs) + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: choose a rank range (e.g., 10-100)\n",
        "# TODO: compute log_ranks and log_freqs\n",
        "# Hint: np.log10 or np.log\n",
        "# TODO: fit a line with np.polyfit(log_ranks, log_freqs, 1)\n",
        "# TODO: print the slope and intercept\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fit line formula (log-log space)**\n",
        "We fit a line in log space and then map it back to frequency space:\n",
        "$$y = m x + b$$\n",
        "$$x = \\log(\\text{rank}), \\quad y = \\log(\\text{freq})$$\n",
        "$$\\text{freq} = 10^{b} \\cdot \\text{rank}^{m}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot log-log Zipf and the fitted line\n",
        "# Hint: plt.loglog for the data\n",
        "# Hint: use the fitted slope/intercept to build the line\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Lab 2 intro - dataset splits and UNK handling\n",
        "We create train/dev/test splits and replace rare tokens with `<UNK>`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About splits and `<UNK>`**\n",
        "- We create train/dev/test splits so we can tune on dev and evaluate on test.\n",
        "- The *train* split is used to build the vocabulary and estimate n-gram counts.\n",
        "- Tokens not in the training vocab are replaced with `<UNK>` to handle unseen words.\n",
        "\n",
        "**Why tune on the dev set?**\n",
        "- Hyperparameters (like `alpha` for smoothing or `k` for sampling) are chosen to\n",
        "  perform well on dev.\n",
        "- We avoid tuning on test to prevent optimistic, biased evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: split into train/dev/test\n",
        "# Hint: ds.train_test_split(test_size=...)\n",
        "# Hint: split the train portion again to make dev\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq=2):\n",
        "    # TODO: return a vocab dict {token: count} filtered by min_freq\n",
        "    # Hint: start from Counter and filter\n",
        "    raise NotImplementedError\n",
        "\n",
        "def replace_unk(tokens, vocab):\n",
        "    # TODO: replace tokens not in vocab with '<UNK>'\n",
        "    raise NotImplementedError\n",
        "\n",
        "# TODO: use train texts to build vocab\n",
        "# TODO: apply replace_unk to a few sample sentences\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Finding a real `<UNK>` replacement**\n",
        "We want an example sentence where at least one token is *not* in the vocab.\n",
        "If this is rare, increase `min_freq` to force more words to become `<UNK>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: find a sentence where replace_unk inserts '<UNK>'\n",
        "# Hint: build a temporary vocab with higher min_freq (e.g., 5 or 10)\n",
        "# Hint: scan train_texts and check if '<UNK>' appears in replaced tokens\n",
        "# Hint: print a window around the first '<UNK>' token (index-5 : index+5)\n",
        "# TODO: print before/after for one example\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: N-gram counts and add-alpha smoothing\n",
        "We build unigram and bigram counts from the training set, then apply add-alpha smoothing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About n-gram counts and smoothing**\n",
        "- An *n-gram* is a contiguous sequence of `n` tokens (unigram n=1, bigram n=2).\n",
        "- We add `<BOS>` and `<EOS>` to mark sentence boundaries for n-gram counting.\n",
        "- Bigram counts from the *train* split define a conditional model: $P(w_i \\mid w_{i-1})$.\n",
        "  This is not gradient training; it is counting-based estimation.\n",
        "- Add-alpha smoothing avoids zero probabilities by adding a small constant to counts:\n",
        "$$P_{\\alpha}(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i) + \\alpha}{C(w_{i-1}) + \\alpha V}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: build train_tokens and dev_tokens using replace_unk\n",
        "# Hint: train_texts = [ex[text_field] for ex in train if ex[text_field].strip()]\n",
        "# Hint: train_tokens = [replace_unk(tokenize_whitespace(t), vocab) for t in train_texts]\n",
        "# Hint: add <BOS> and <EOS> for n-gram counts\n",
        "\n",
        "# TODO: compute unigram_counts and bigram_counts with Counter\n",
        "# Hint: bigrams can be pairs from zip(seq[:-1], seq[1:])\n",
        "\n",
        "# TODO: define bigram_prob(prev, tok, alpha) with add-alpha smoothing\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cadd0ca5",
      "metadata": {},
      "source": [
        "## Step 6: Perplexity and simple grid search\n",
        "We evaluate on the dev split and tune the smoothing strength by trying a few alphas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140081e4",
      "metadata": {},
      "source": [
        "**About perplexity**\n",
        "- Perplexity is the exponent of the average negative log probability.\n",
        "- The minus sign turns log-likelihoods (which are negative) into a positive surprisal value.\n",
        "- The `-1/N` factor averages per token so results are comparable across lengths.\n",
        "- With log, the formula is:\n",
        "$$\\mathrm{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^N \\log p(w_i \\mid w_{i-1})\\right)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6736f9dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement perplexity for a list of token sequences\n",
        "# Hint: use log probabilities and average over total bigrams\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03c1a16",
      "metadata": {},
      "source": [
        "**Why grid search?**\n",
        "- We do not know the best alpha in advance.\n",
        "- A small grid search tries a few candidate values and picks the one\n",
        "  with the lowest dev perplexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e7fa04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: grid search over a few alpha values (e.g., [0.1, 0.5, 1.0])\n",
        "# TODO: print dev perplexity for each and pick the best\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Top-k sampling\n",
        "We generate short samples using a bigram model and top-k sampling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About top-k sampling**\n",
        "- We keep only the top-k most likely next tokens.\n",
        "- Sampling from this trimmed distribution balances variety and coherence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement sample_next(prev, alpha, k) using bigram_prob\n",
        "# Hint: compute probabilities for all tokens, take top-k, sample with np.random.choice\n",
        "\n",
        "# TODO: implement generate(max_len, alpha, k) starting from <BOS>\n",
        "# TODO: print 2-3 generated samples\n",
        "\n",
        "# Write your code below\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}