{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1 + Lab 2 Lesson 2: BPE, Zipf slope, and n-gram basics\n",
        "\n",
        "**What we use**\n",
        "- `datasets` (Hugging Face) for loading and splitting datasets.\n",
        "- `transformers` (Hugging Face) for BPE tokenization via `AutoTokenizer`.\n",
        "- `numpy` for log-log slope fitting and random sampling utilities.\n",
        "- `Counter` for frequency counts.\n",
        "\n",
        "**Goals**\n",
        "- Apply BPE tokenization and inspect subword behavior.\n",
        "- Fit a Zipf slope on log-log axes and interpret it.\n",
        "- Start Lab 2: create train/dev/test splits and handle unknown tokens.\n",
        "- Build n-gram counts, add smoothing, and compute perplexity.\n",
        "- Generate short samples with top-k sampling.\n",
        "\n",
        "**Structure**\n",
        "1) Load a dataset subset.\n",
        "2) BPE tokenization practice.\n",
        "3) Zipf slope fitting.\n",
        "4) Lab 2 intro: split + UNK handling.\n",
        "5) N-gram counts + add-alpha smoothing.\n",
        "6) Perplexity + simple grid search.\n",
        "7) Top-k sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c12a4f01",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed84e83d",
      "metadata": {},
      "source": [
        "**Why the fixed seed matters here**\n",
        "- It makes `train_test_split` reproducible when we create train/dev/test.\n",
        "- It stabilizes `np.random.choice` in top-k sampling so examples are repeatable.\n",
        "- It does not change the dataset content itself, only the randomized operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033f202a",
      "metadata": {},
      "source": [
        "## Step 1: Load a dataset subset\n",
        "Choose a dataset and a text field. Keep a small subset for speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d7e4e589",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': Value('string'), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'])}\n",
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        }
      ],
      "source": [
        "# Dataset choice\n",
        "# Examples: name = 'ag_news' (text), 'imdb' (text), 'yelp_polarity' (text)\n",
        "name = 'ag_news'\n",
        "config = None\n",
        "text_field = 'text'\n",
        "\n",
        "if config:\n",
        "    ds = load_dataset(name, config, split='train')\n",
        "else:\n",
        "    ds = load_dataset(name, split='train')\n",
        "\n",
        "subset = ds.select(range(2000))\n",
        "print(subset.features)\n",
        "print(subset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9fd38494",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "---\n",
            "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "---\n",
            "Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print('---')\n",
        "    print(subset[i][text_field])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bcf5ff6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample texts: 2000\n"
          ]
        }
      ],
      "source": [
        "texts = [ex[text_field] for ex in subset if ex[text_field].strip()]\n",
        "print('sample texts:', len(texts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5131cef6",
      "metadata": {},
      "source": [
        "## Step 2: BPE tokenization\n",
        "We use a pretrained BPE tokenizer to see how subwords split words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef907622",
      "metadata": {},
      "source": [
        "**About `AutoTokenizer` and BPE**\n",
        "- `AutoTokenizer` loads a pretrained tokenizer from the Hugging Face Hub.\n",
        "- For GPT-2, this is a Byte-Pair Encoding (BPE) tokenizer that splits words into subwords.\n",
        "- `tokenizer(text)` returns `input_ids` (integers), and `convert_ids_to_tokens` shows subword pieces.\n",
        "- We set `add_special_tokens=False` to keep the raw segmentation visible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09da17e",
      "metadata": {},
      "source": [
        "**What is BPE?**\n",
        "Byte-Pair Encoding (BPE) builds a subword vocabulary by repeatedly merging\n",
        "the most frequent *adjacent* symbol pairs. Symbols start as characters and\n",
        "become longer subwords after merges.\n",
        "\n",
        "BPE is an iterative process: after each merge, pairs are re-counted and the next most frequent pair is merged.\n",
        "\n",
        "Example (simplified):\n",
        "$$t\\ h\\ e\\ r\\ e \\xrightarrow{(t,h)} th\\ e\\ r\\ e \\xrightarrow{(th,e)} the\\ r\\ e$$\n",
        "Here `(t,h)` is chosen because it is the most frequent adjacent pair in the corpus at that step;\n",
        "then `(th,e)` becomes frequent and is merged next. That is why the tokens become `the | r | e`.\n",
        "\n",
        "**Can we compute Zipf on BPE tokens?** Yes. If you count BPE tokens instead of words,\n",
        "you can make a Zipf plot for subword units. The curve shape can change because the vocabulary\n",
        "now includes short subword pieces.\n",
        "\n",
        "**Where BPE is used**\n",
        "BPE tokenization is the standard input step for many pretrained transformer models.\n",
        "In later labs, we will feed BPE token IDs into models and compare tokenization effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "c53dfaf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d361ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cd6eb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06992777",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\b\n",
            "BPE tokens ['Wall', 'Ä St', '.', 'Ä Bears', 'Ä Claw', 'Ä Back', 'Ä Into', 'Ä the', 'Ä Black', 'Ä (', 'Reuters', ')', 'Ä Reuters', 'Ä -', 'Ä Short', '-', 'sell', 'ers', ',', 'Ä Wall', 'Ä Street', \"'s\", 'Ä dwindling', '\\\\', 'band', 'Ä of', 'Ä ultra', '-', 'cy', 'n']\n",
            "dec Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "dummy join Wall Ä St . Ä Bears Ä Claw Ä Back Ä Into Ä the Ä Black Ä ( Reuters ) Ä Reuters Ä - Ä Short - sell ers , Ä Wall Ä \n",
            "\n",
            "text Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\n",
            "BPE tokens ['Car', 'ly', 'le', 'Ä Looks', 'Ä Tow', 'ard', 'Ä Commercial', 'Ä Aerospace', 'Ä (', 'Reuters', ')', 'Ä Reuters', 'Ä -', 'Ä Private', 'Ä investment', 'Ä firm', 'Ä Carly', 'le', 'Ä Group', ',', '\\\\', 'which', 'Ä has', 'Ä a', 'Ä reputation', 'Ä for', 'Ä making', 'Ä well', '-', 'tim']\n",
            "dec Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "dummy join Car ly le Ä Looks Ä Tow ard Ä Commercial Ä Aerospace Ä ( Reuters ) Ä Reuters Ä - Ä Private Ä investment Ä firm\n",
            "\n",
            "text Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about th\n",
            "BPE tokens ['Oil', 'Ä and', 'Ä Economy', 'Ä Cloud', 'Ä St', 'ocks', \"'\", 'Ä Outlook', 'Ä (', 'Reuters', ')', 'Ä Reuters', 'Ä -', 'Ä So', 'aring', 'Ä crude', 'Ä prices', 'Ä plus', 'Ä worries', '\\\\', 'about', 'Ä the', 'Ä economy', 'Ä and', 'Ä the', 'Ä outlook', 'Ä for', 'Ä earnings', 'Ä are', 'Ä expected']\n",
            "dec Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "dummy join Oil Ä and Ä Economy Ä Cloud Ä St ocks ' Ä Outlook Ä ( Reuters ) Ä Reuters Ä - Ä So aring Ä crude Ä prices Ä plus\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample = texts[:3]\n",
        "\n",
        "for s in sample:\n",
        "    enc = tokenizer(s, add_special_tokens=False)\n",
        "    ids = enc[\"input_ids\"]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "    dec = tokenizer.decode(ids)\n",
        "    print(\"text\", s[:100])\n",
        "    print(\"BPE tokens\", tokens[:30])\n",
        "    print(\"dec\", dec)\n",
        "    print(\"dummy join\", ' '.join(tokens)[:100])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fdae0b8",
      "metadata": {},
      "source": [
        "## Step 3: Zipf slope fitting\n",
        "We fit a line to log(rank) vs log(freq) for ranks 10-100 to estimate the slope.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1327e48",
      "metadata": {},
      "source": [
        "**About Zipf slope fitting**\n",
        "- We compute token frequencies from a simple whitespace tokenizer.\n",
        "- On a log-log plot, Zipf-like data forms an approximate straight line.\n",
        "- We fit only a middle rank range (start/end) to avoid very frequent function words\n",
        "  at the head and sparse, noisy counts in the tail.\n",
        "- The slope is negative; a less steep slope often suggests higher lexical diversity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e949590f",
      "metadata": {},
      "source": [
        "**How `np.polyfit` works here**\n",
        "- `np.polyfit(x, y, 1)` fits a line `y = m*x + b` by least squares.\n",
        "- It returns `[m, b]` where `m` is the slope and `b` is the intercept.\n",
        "- On log-log data, the slope estimates the Zipf exponent.\n",
        "- We fit only a middle rank range to reduce head/tail distortion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "18d8e69b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_whitespace(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def get_token_counts(texts):\n",
        "    counts = Counter()\n",
        "    for t in texts:\n",
        "        counts.update(tokenize_whitespace(t))\n",
        "    return counts\n",
        "\n",
        "texts = [ex[text_field] for ex in subset if ex[text_field].strip()]\n",
        "counts = get_token_counts(texts)\n",
        "freqs = sorted(counts.values(), reverse=True)\n",
        "ranks = np.arange(1, len(freqs) + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e6772b06",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: choose a rank range (e.g., 10-100)\n",
        "# TODO: compute log_ranks and log_freqs\n",
        "# Hint: np.log10 or np.log\n",
        "# TODO: fit a line with np.polyfit(log_ranks, log_freqs, 1)\n",
        "# TODO: print the slope and intercept\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b62668b",
      "metadata": {},
      "source": [
        "**Fit line formula (log-log space)**\n",
        "We fit a line in log space and then map it back to frequency space:\n",
        "$$y = m x + b$$\n",
        "$$x = \\log(\\text{rank}), \\quad y = \\log(\\text{freq})$$\n",
        "$$\\text{freq} = 10^{b} \\cdot \\text{rank}^{m}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2ec025",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot log-log Zipf and the fitted line\n",
        "# Hint: plt.loglog for the data\n",
        "# Hint: use the fitted slope/intercept to build the line\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b0ff71",
      "metadata": {},
      "source": [
        "## Step 4: Lab 2 intro - dataset splits and UNK handling\n",
        "We create train/dev/test splits and replace rare tokens with `<UNK>`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f704049",
      "metadata": {},
      "source": [
        "**About splits and `<UNK>`**\n",
        "- We create train/dev/test splits so we can tune on dev and evaluate on test.\n",
        "- The *train* split is used to build the vocabulary and estimate n-gram counts.\n",
        "- Tokens not in the training vocab are replaced with `<UNK>` to handle unseen words.\n",
        "\n",
        "**Why tune on the dev set?**\n",
        "- Hyperparameters (like `alpha` for smoothing or `k` for sampling) are chosen to\n",
        "  perform well on dev.\n",
        "- We avoid tuning on test to prevent optimistic, biased evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8b880060",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 76800\n",
            "test 24000\n",
            "dev 19200\n"
          ]
        }
      ],
      "source": [
        "# TODO: split into train/dev/test\n",
        "# Hint: ds.train_test_split(test_size=...)\n",
        "# Hint: split the train portion again to make dev\n",
        "\n",
        "# Write your code below\n",
        "\n",
        "\n",
        "split = ds.train_test_split(test_size=0.2, seed=SEED)\n",
        "\n",
        "train_full = split[\"train\"]\n",
        "test = split[\"test\"]\n",
        "\n",
        "train_split = train_full.train_test_split(test_size=0.2, seed=SEED)\n",
        "train = train_split[\"train\"]\n",
        "dev = train_split[\"test\"]\n",
        "\n",
        "\n",
        "print(\"train\", len(train))\n",
        "print(\"test\", len(test))\n",
        "print(\"dev\", len(dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d8ec43ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_whitespace(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def get_token_counts(texts):\n",
        "    counts = Counter()\n",
        "    for t in texts:\n",
        "        counts.update(tokenize_whitespace(t))\n",
        "    return counts\n",
        "\n",
        "def extract_texts(subset):\n",
        "    return [ex[text_field] for ex in subset if ex[text_field].strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "bd94dcea",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[292, 67, 292]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"asdas\")['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "99289ccf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "bc53bc9b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_values([1, 12])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9449e0c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'> <class 'dict'> <class 'str'>\n",
            "a 1\n",
            "1\n",
            "<class 'str'> <class 'dict'> <class 'str'>\n",
            "b 12\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "d = {\"a\": 1, \"b\": 12}\n",
        "for key in d:\n",
        "    print(type(key), type(d), type(\"a\"))\n",
        "    print(key, d[key])\n",
        "    print(d[\"a\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b7d0e573",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dict_keys(['a', 'b']),\n",
              " dict_values([1, 12]),\n",
              " dict_items([('a', 1), ('b', 12)]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d.keys(), d.values(), d.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c44c3d08",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a 1\n",
            "b 12\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'a': 1}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d = {\"a\": 1, \"b\": 12}\n",
        "for key, value in d.items():\n",
        "    print(key, value)\n",
        "\n",
        "\n",
        "{key: value for key, value in d.items() if value < 10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ac1e8cc2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 76800\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e1c4f74c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mextract_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "\u001b[0;32mdef\u001b[0m \u001b[0mextract_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_10156/1411551957.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "??extract_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04648436",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq=2):\n",
        "    # TODO: return a vocab dict {token: count} filtered by min_freq\n",
        "    # Hint: start from Counter and filter\n",
        "    counts = Counter()\n",
        "    for text in texts:\n",
        "        counts.update(tokenizer(text)['input_ids'])\n",
        "    return Counter({token_id: count for token_id, count in counts.items() if count >= min_freq})\n",
        "    \n",
        "# type annotations\n",
        "def replace_unk(tokens: list[int], vocab: Counter) -> list[int]:\n",
        "    # TODO: replace tokens not in vocab with '<UNK>'\n",
        "    filtered = []\n",
        "    for token in tokens:\n",
        "        if token not in vocab:\n",
        "            filtered.append(tokenizer.unk_token_id)\n",
        "        else:\n",
        "            filtered.append(token)\n",
        "    return filtered\n",
        "\n",
        "\n",
        "\n",
        "# TODO: use train texts to build vocab\n",
        "# TODO: apply replace_unk to a few sample sentences\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c02bd8e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = build_vocab(extract_texts(train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a032ad75",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c58911",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ASj-fA2SDq hello world'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_sentence = \"JADASDAS hello world\"\n",
        "test_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c8c45c35",
      "metadata": {},
      "outputs": [],
      "source": [
        "before = tokenizer(test_sentence)[\"input_ids\"]\n",
        "after = replace_unk(before, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "d4def1fc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1921, 73, 12, 69, 32, 17, 10305, 80, 23748, 995],\n",
              " [1921, 73, 12, 69, 32, 17, 10305, 80, 23748, 995])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "before[:10], after[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "fbb92566",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(13, 126136),\n",
              " (262, 114031),\n",
              " (11, 102443),\n",
              " (284, 75474),\n",
              " (257, 63124),\n",
              " (286, 62716),\n",
              " (287, 59281),\n",
              " (26, 55133),\n",
              " (12, 51757),\n",
              " (290, 43971)]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "7f3f8562",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[13, 262, 11, 284, 257, 286, 287, 26, 12, 290]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[key for key, value in vocab.most_common(10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "cd78db06",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.', 'Ä the', ',', 'Ä to', 'Ä a', 'Ä of', 'Ä in', ';', '-', 'Ä and']"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens([token_id for token_id, count in vocab.most_common(10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e00c779",
      "metadata": {},
      "source": [
        "**Finding a real `<UNK>` replacement**\n",
        "We want an example sentence where at least one token is *not* in the vocab.\n",
        "If this is rare, increase `min_freq` to force more words to become `<UNK>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70faa12b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: find a sentence where replace_unk inserts '<UNK>'\n",
        "# Hint: build a temporary vocab with higher min_freq (e.g., 5 or 10)\n",
        "# Hint: scan train_texts and check if '<UNK>' appears in replaced tokens\n",
        "# Hint: print a window around the first '<UNK>' token (index-5 : index+5)\n",
        "# TODO: print before/after for one example\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066ecc86",
      "metadata": {},
      "source": [
        "**Homework (Lesson 2)**\n",
        "- Use the `wikitext` dataset (`wikitext-2-raw-v1`).\n",
        "- Load the GPT-2 tokenizer with `AutoTokenizer`.\n",
        "- Build a vocabulary with `build_vocab` **over GPT-2 tokens** over token ids.\n",
        "- Get the top 1000 tokens by frequency.\n",
        "- Hint: use `Counter.most_common(1000)` instead of manual sorting.\n",
        "- Tokenize the first 5 texts with GPT-2, keep only tokens in the top 1000,\n",
        "  then reconstruct with `tokenizer.decode(...)`.\n",
        "- Compare the original vs reconstructed texts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "994d2bca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: load wikitext-2-raw-v1 train split\n",
        "# TODO: load GPT-2 tokenizer with AutoTokenizer\n",
        "# TODO: build vocab with build_vocab over GPT-2 tokens\n",
        "# Hint: convert ids to tokens and join with spaces before calling build_vocab\n",
        "# TODO: get top 1000 tokens by frequency\n",
        "# TODO: tokenize first 5 texts, keep only tokens from top-1000, then reconstruct using tokenizer.decode\n",
        "# TODO: print original vs reconstructed texts\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7e007e",
      "metadata": {},
      "source": [
        "## Step 5: N-gram counts and add-alpha smoothing\n",
        "We build unigram and bigram counts from the training set, then apply add-alpha smoothing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5da9947",
      "metadata": {},
      "source": [
        "**About n-gram counts and smoothing**\n",
        "- An *n-gram* is a contiguous sequence of `n` tokens (unigram n=1, bigram n=2).\n",
        "- We add `<BOS>` and `<EOS>` to mark sentence boundaries for n-gram counting.\n",
        "- Bigram counts from the *train* split define a conditional model: $P(w_i \\mid w_{i-1})$.\n",
        "  This is not gradient training; it is counting-based estimation.\n",
        "- Add-alpha smoothing avoids zero probabilities by adding a small constant to counts:\n",
        "$$P_{\\alpha}(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i) + \\alpha}{C(w_{i-1}) + \\alpha V}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "dbc7ba48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on GPT2TokenizerFast in module transformers.models.gpt2.tokenization_gpt2_fast object:\n",
            "\n",
            "class GPT2TokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
            " |  GPT2TokenizerFast(vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
            " |\n",
            " |  Construct a \"fast\" GPT-2 tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n",
            " |  Byte-Pair-Encoding.\n",
            " |\n",
            " |  This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
            " |  be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
            " |\n",
            " |  ```python\n",
            " |  >>> from transformers import GPT2TokenizerFast\n",
            " |\n",
            " |  >>> tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n",
            " |  >>> tokenizer(\"Hello world\")[\"input_ids\"]\n",
            " |  [15496, 995]\n",
            " |\n",
            " |  >>> tokenizer(\" Hello world\")[\"input_ids\"]\n",
            " |  [18435, 995]\n",
            " |  ```\n",
            " |\n",
            " |  You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n",
            " |  the model was not pretrained this way, it might yield a decrease in performance.\n",
            " |\n",
            " |  <Tip>\n",
            " |\n",
            " |  When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n",
            " |\n",
            " |  </Tip>\n",
            " |\n",
            " |  This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
            " |  refer to this superclass for more information regarding those methods.\n",
            " |\n",
            " |  Args:\n",
            " |      vocab_file (`str`, *optional*):\n",
            " |          Path to the vocabulary file.\n",
            " |      merges_file (`str`, *optional*):\n",
            " |          Path to the merges file.\n",
            " |      tokenizer_file (`str`, *optional*):\n",
            " |          Path to [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n",
            " |          contains everything needed to load the tokenizer.\n",
            " |      unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
            " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
            " |          token instead.\n",
            " |      bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
            " |          The beginning of sequence token.\n",
            " |      eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
            " |          The end of sequence token.\n",
            " |      add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
            " |          other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      GPT2TokenizerFast\n",
            " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
            " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
            " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
            " |      transformers.utils.hub.PushToHubMixin\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]\n",
            " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
            " |\n",
            " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
            " |      [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.\n",
            " |\n",
            " |      Args:\n",
            " |          save_directory (`str`):\n",
            " |              The directory in which to save the vocabulary.\n",
            " |          filename_prefix (`str`, *optional*):\n",
            " |              An optional prefix to add to the named of the saved files.\n",
            " |\n",
            " |      Returns:\n",
            " |          `tuple(str)`: Paths to the files saved.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __annotations__ = {}\n",
            " |\n",
            " |  model_input_names = ['input_ids', 'attention_mask']\n",
            " |\n",
            " |  slow_tokenizer_class = <class 'transformers.models.gpt2.tokenization_g...\n",
            " |      Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
            " |\n",
            " |      This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
            " |      be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
            " |\n",
            " |      ```python\n",
            " |      >>> from transformers import GPT2Tokenizer\n",
            " |\n",
            " |      >>> tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
            " |      >>> tokenizer(\"Hello world\")[\"input_ids\"]\n",
            " |      [15496, 995]\n",
            " |\n",
            " |      >>> tokenizer(\" Hello world\")[\"input_ids\"]\n",
            " |      [18435, 995]\n",
            " |      ```\n",
            " |\n",
            " |      You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
            " |      call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
            " |      this superclass for more information regarding those methods.\n",
            " |\n",
            " |      Args:\n",
            " |          vocab_file (`str`):\n",
            " |              Path to the vocabulary file.\n",
            " |          merges_file (`str`):\n",
            " |              Path to the merges file.\n",
            " |          errors (`str`, *optional*, defaults to `\"replace\"`):\n",
            " |              Paradigm to follow when decoding bytes to UTF-8. See\n",
            " |              [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
            " |          unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
            " |              The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
            " |              token instead.\n",
            " |          bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
            " |              The beginning of sequence token.\n",
            " |          eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
            " |              The end of sequence token.\n",
            " |          pad_token (`str`, *optional*):\n",
            " |              The token used for padding, for example when batching sequences of different lengths.\n",
            " |          add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
            " |              other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
            " |          add_bos_token (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to add an initial beginning of sentence token to the input. This allows to treat the leading\n",
            " |              word just as any other word.\n",
            " |\n",
            " |\n",
            " |  vocab_files_names = {'merges_file': 'merges.txt', 'tokenizer_file': 't...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
            " |\n",
            " |  __bool__(self) -> bool\n",
            " |      Returns True, to avoid expensive `assert tokenizer` gotchas.\n",
            " |\n",
            " |  __len__(self) -> int\n",
            " |      Size of the full vocabulary with the added tokens.\n",
            " |\n",
            " |  convert_ids_to_tokens(self, ids: Union[int, list[int]], skip_special_tokens: bool = False) -> Union[str, list[str]]\n",
            " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
            " |      added tokens.\n",
            " |\n",
            " |      Args:\n",
            " |          ids (`int` or `list[int]`):\n",
            " |              The token id (or token ids) to convert to tokens.\n",
            " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to remove special tokens in the decoding.\n",
            " |\n",
            " |      Returns:\n",
            " |          `str` or `list[str]`: The decoded token(s).\n",
            " |\n",
            " |  convert_tokens_to_ids(self, tokens: Union[str, collections.abc.Iterable[str]]) -> Union[int, list[int]]\n",
            " |      Converts a token string (or a sequence of tokens) in a single integer id (or a Iterable of ids), using the\n",
            " |      vocabulary.\n",
            " |\n",
            " |      Args:\n",
            " |          tokens (`str` or `Iterable[str]`): One or several token(s) to convert to token id(s).\n",
            " |\n",
            " |      Returns:\n",
            " |          `int` or `list[int]`: The token id or list of token ids.\n",
            " |\n",
            " |  convert_tokens_to_string(self, tokens: list[str]) -> str\n",
            " |      Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n",
            " |      often want to remove sub-word tokenization artifacts at the same time.\n",
            " |\n",
            " |      Args:\n",
            " |          tokens (`list[str]`): The token to join in a string.\n",
            " |\n",
            " |      Returns:\n",
            " |          `str`: The joined tokens.\n",
            " |\n",
            " |  get_added_vocab(self) -> dict[str, int]\n",
            " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
            " |\n",
            " |      Returns:\n",
            " |          `dict[str, int]`: The added tokens.\n",
            " |\n",
            " |  get_vocab(self) -> dict[str, int]\n",
            " |      Returns the vocabulary as a dictionary of token to index.\n",
            " |\n",
            " |      `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the\n",
            " |      vocab.\n",
            " |\n",
            " |      Returns:\n",
            " |          `dict[str, int]`: The vocabulary.\n",
            " |\n",
            " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
            " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\n",
            " |      this inside your training loop.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          pair (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
            " |              sequence.\n",
            " |\n",
            " |      Returns:\n",
            " |          `int`: Number of special tokens added to sequences.\n",
            " |\n",
            " |  set_truncation_and_padding(self, padding_strategy: transformers.utils.generic.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int], padding_side: Optional[str])\n",
            " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
            " |      library) and restore the tokenizer settings afterwards.\n",
            " |\n",
            " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
            " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
            " |      section.\n",
            " |\n",
            " |      Args:\n",
            " |          padding_strategy ([`~utils.PaddingStrategy`]):\n",
            " |              The kind of padding that will be applied to the input\n",
            " |          truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):\n",
            " |              The kind of truncation that will be applied to the input\n",
            " |          max_length (`int`):\n",
            " |              The maximum size of a sequence.\n",
            " |          stride (`int`):\n",
            " |              The stride to use when handling overflow.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
            " |              the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |\n",
            " |  tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> list[str]\n",
            " |      Converts a string into a sequence of tokens, replacing unknown tokens with the `unk_token`.\n",
            " |\n",
            " |      Args:\n",
            " |          text (`str`):\n",
            " |              The sequence to be encoded.\n",
            " |          pair (`str`, *optional*):\n",
            " |              A second sequence to be encoded with the first.\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to add the special tokens associated with the corresponding model.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the underlying model specific encode method. See details in\n",
            " |              [`~PreTrainedTokenizerBase.__call__`]\n",
            " |\n",
            " |      Returns:\n",
            " |          `list[str]`: The list of tokens.\n",
            " |\n",
            " |  train_new_from_iterator(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
            " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
            " |      as the current one.\n",
            " |\n",
            " |      Args:\n",
            " |          text_iterator (generator of `list[str]`):\n",
            " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
            " |              if you have everything in memory.\n",
            " |          vocab_size (`int`):\n",
            " |              The size of the vocabulary you want for your tokenizer.\n",
            " |          length (`int`, *optional*):\n",
            " |              The total number of sequences in the iterator. This is used to provide meaningful progress tracking\n",
            " |          new_special_tokens (list of `str` or `AddedToken`, *optional*):\n",
            " |              A list of new special tokens to add to the tokenizer you are training.\n",
            " |          special_tokens_map (`dict[str, str]`, *optional*):\n",
            " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
            " |              token name to new special token name in this argument.\n",
            " |          kwargs (`dict[str, Any]`, *optional*):\n",
            " |              Additional keyword arguments passed along to the trainer from the ðŸ¤— Tokenizers library.\n",
            " |\n",
            " |      Returns:\n",
            " |          [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on\n",
            " |          `text_iterator`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
            " |\n",
            " |  added_tokens_decoder\n",
            " |      Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.\n",
            " |\n",
            " |      Returns:\n",
            " |          `dict[str, int]`: The added tokens.\n",
            " |\n",
            " |  added_tokens_encoder\n",
            " |      Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\n",
            " |      optimisation in `self._added_tokens_encoder` for the slow tokenizers.\n",
            " |\n",
            " |  backend_tokenizer\n",
            " |      `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
            " |\n",
            " |  can_save_slow_tokenizer\n",
            " |      `bool`: Whether or not the slow tokenizer can be saved. For a sentencepiece based slow tokenizer, this\n",
            " |      can only be `True` if the original `\"sentencepiece.model\"` was not deleted.\n",
            " |\n",
            " |  decoder\n",
            " |      `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
            " |\n",
            " |  is_fast\n",
            " |\n",
            " |  vocab\n",
            " |\n",
            " |  vocab_size\n",
            " |      `int`: Size of the base vocabulary (without the added tokens).\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |\n",
            " |  __call__(self, text: Union[str, list[str], list[list[str]], NoneType] = None, text_pair: Union[str, list[str], list[list[str]], NoneType] = None, text_target: Union[str, list[str], list[list[str]], NoneType] = None, text_pair_target: Union[str, list[str], list[list[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
            " |      sequences.\n",
            " |\n",
            " |      Args:\n",
            " |          text (`str`, `list[str]`, `list[list[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |          text_pair (`str`, `list[str]`, `list[list[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |          text_target (`str`, `list[str]`, `list[list[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
            " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
            " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |          text_pair_target (`str`, `list[str]`, `list[list[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
            " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
            " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence is provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |\n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |\n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |\n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |\n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |\n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |\n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |\n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |\n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |\n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |\n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  apply_chat_template(self, conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]], tools: Optional[list[Union[dict, Callable]]] = None, documents: Optional[list[dict[str, str]]] = None, chat_template: Optional[str] = None, add_generation_prompt: bool = False, continue_final_message: bool = False, tokenize: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_dict: bool = False, return_assistant_tokens_mask: bool = False, tokenizer_kwargs: Optional[dict[str, Any]] = None, **kwargs) -> Union[str, list[int], list[str], list[list[int]], transformers.tokenization_utils_base.BatchEncoding]\n",
            " |      Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
            " |      ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
            " |      determine the format and control tokens to use when converting.\n",
            " |\n",
            " |      Args:\n",
            " |          conversation (Union[list[dict[str, str]], list[list[dict[str, str]]]]): A list of dicts\n",
            " |              with \"role\" and \"content\" keys, representing the chat history so far.\n",
            " |          tools (`list[Union[Dict, Callable]]`, *optional*):\n",
            " |              A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
            " |              support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
            " |              giving the name, description and argument types for the tool. See our\n",
            " |              [tool use guide](https://huggingface.co/docs/transformers/en/chat_extras#passing-tools)\n",
            " |              for more information.\n",
            " |          documents (`list[dict[str, str]]`, *optional*):\n",
            " |              A list of dicts representing documents that will be accessible to the model if it is performing RAG\n",
            " |              (retrieval-augmented generation). If the template does not support RAG, this argument will have no\n",
            " |              effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.\n",
            " |          chat_template (`str`, *optional*):\n",
            " |              A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\n",
            " |              argument, as the model's template will be used by default.\n",
            " |          add_generation_prompt (bool, *optional*):\n",
            " |              If this is set, a prompt with the token(s) that indicate\n",
            " |              the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\n",
            " |              Note that this argument will be passed to the chat template, and so it must be supported in the\n",
            " |              template for this argument to have any effect.\n",
            " |          continue_final_message (bool, *optional*):\n",
            " |              If this is set, the chat will be formatted so that the final\n",
            " |              message in the chat is open-ended, without any EOS tokens. The model will continue this message\n",
            " |              rather than starting a new one. This allows you to \"prefill\" part of\n",
            " |              the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n",
            " |          tokenize (`bool`, defaults to `True`):\n",
            " |              Whether to tokenize the output. If `False`, the output will be a string.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
            " |               index) among:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, defaults to `False`):\n",
            " |              Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
            " |              not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
            " |              values are:\n",
            " |              - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return NumPy `np.ndarray` objects.\n",
            " |              - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
            " |          return_dict (`bool`, defaults to `False`):\n",
            " |              Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n",
            " |          tokenizer_kwargs (`dict[str: Any]`, *optional*): Additional kwargs to pass to the tokenizer.\n",
            " |          return_assistant_tokens_mask (`bool`, defaults to `False`):\n",
            " |              Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n",
            " |              the mask will contain 1. For user and system tokens, the mask will contain 0.\n",
            " |              This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n",
            " |          **kwargs: Additional kwargs to pass to the template renderer. Will be accessible by the chat template.\n",
            " |\n",
            " |      Returns:\n",
            " |          `Union[list[int], Dict]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
            " |          output is ready to pass to the model, either directly or via methods like `generate()`. If `return_dict` is\n",
            " |          set, will return a dict of tokenizer outputs instead.\n",
            " |\n",
            " |  as_target_tokenizer(self)\n",
            " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
            " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
            " |\n",
            " |  batch_decode(self, sequences: Union[list[int], list[list[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: Optional[bool] = None, **kwargs) -> list[str]\n",
            " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
            " |\n",
            " |      Args:\n",
            " |          sequences (`Union[list[int], list[list[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
            " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
            " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to remove special tokens in the decoding.\n",
            " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
            " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
            " |              `self.clean_up_tokenization_spaces`.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the underlying model specific decode method.\n",
            " |\n",
            " |      Returns:\n",
            " |          `list[str]`: The list of decoded sentences.\n",
            " |\n",
            " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[list[str], list[tuple[str, str]], list[list[str]], list[tuple[list[str], list[str]]], list[list[int]], list[tuple[list[int], list[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, split_special_tokens: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
            " |\n",
            " |      <Tip warning={true}>\n",
            " |\n",
            " |      This method is deprecated, `__call__` should be used instead.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          batch_text_or_text_pairs (`list[str]`, `list[tuple[str, str]]`, `list[list[str]]`, `list[tuple[list[str], list[str]]]`, and for not-fast tokenizers, also `list[list[int]]`, `list[tuple[list[int], list[int]]]`):\n",
            " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
            " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
            " |              details in `encode_plus`).\n",
            " |\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence is provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |\n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |\n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |\n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |\n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |\n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |\n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |\n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |\n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |\n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |\n",
            " |  build_inputs_with_special_tokens(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None) -> list[int]\n",
            " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
            " |      adding special tokens.\n",
            " |\n",
            " |      This implementation does not add special tokens and this method should be overridden in a subclass.\n",
            " |\n",
            " |      Args:\n",
            " |          token_ids_0 (`list[int]`): The first tokenized sequence.\n",
            " |          token_ids_1 (`list[int]`, *optional*): The second tokenized sequence.\n",
            " |\n",
            " |      Returns:\n",
            " |          `list[int]`: The model input with special tokens.\n",
            " |\n",
            " |  create_token_type_ids_from_sequences(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None) -> list[int]\n",
            " |      Create the token type IDs corresponding to the sequences passed. [What are token type\n",
            " |      IDs?](../glossary#token-type-ids)\n",
            " |\n",
            " |      Should be overridden in a subclass if the model has a special way of building those.\n",
            " |\n",
            " |      Args:\n",
            " |          token_ids_0 (`list[int]`): The first tokenized sequence.\n",
            " |          token_ids_1 (`list[int]`, *optional*): The second tokenized sequence.\n",
            " |\n",
            " |      Returns:\n",
            " |          `list[int]`: The token type ids.\n",
            " |\n",
            " |  decode(self, token_ids: Union[int, list[int], numpy.ndarray, ForwardRef('torch.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: Optional[bool] = None, **kwargs) -> str\n",
            " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
            " |      tokens and clean up tokenization spaces.\n",
            " |\n",
            " |      Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
            " |\n",
            " |      Args:\n",
            " |          token_ids (`Union[int, list[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
            " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
            " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to remove special tokens in the decoding.\n",
            " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
            " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
            " |              `self.clean_up_tokenization_spaces`.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the underlying model specific decode method.\n",
            " |\n",
            " |      Returns:\n",
            " |          `str`: The decoded sentence.\n",
            " |\n",
            " |  encode(self, text: Union[str, list[str], list[int]], text_pair: Union[str, list[str], list[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -> list[int]\n",
            " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
            " |\n",
            " |      Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
            " |\n",
            " |      Args:\n",
            " |          text (`str`, `list[str]` or `list[int]`):\n",
            " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
            " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |          text_pair (`str`, `list[str]` or `list[int]`, *optional*):\n",
            " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
            " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence is provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |\n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |\n",
            " |          **kwargs: Passed along to the `.tokenize()` method.\n",
            " |\n",
            " |      Returns:\n",
            " |          `list[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\n",
            " |\n",
            " |  encode_message_with_chat_template(self, message: dict[str, str], conversation_history: Optional[list[dict[str, str]]] = None, **kwargs) -> list[int]\n",
            " |      Tokenize a single message. This method is a convenience wrapper around `apply_chat_template` that allows you\n",
            " |      to tokenize messages one by one. This is useful for things like token-by-token streaming.\n",
            " |      This method is not guaranteed to be perfect. For some models, it may be impossible to robustly tokenize\n",
            " |      single messages. For example, if the chat template adds tokens after each message, but also has a prefix that\n",
            " |      is added to the entire chat, it will be impossible to distinguish a chat-start-token from a message-start-token.\n",
            " |      In these cases, this method will do its best to find the correct tokenization, but it may not be perfect.\n",
            " |      **Note:** This method does not support `add_generation_prompt`. If you want to add a generation prompt,\n",
            " |      you should do it separately after tokenizing the conversation.\n",
            " |      Args:\n",
            " |          message (`dict`):\n",
            " |              A dictionary with \"role\" and \"content\" keys, representing the message to tokenize.\n",
            " |          conversation_history (`list[dict]`, *optional*):\n",
            " |              A list of dicts with \"role\" and \"content\" keys, representing the chat history so far. If you are\n",
            " |              tokenizing messages one by one, you should pass the previous messages in the conversation here.\n",
            " |          **kwargs:\n",
            " |              Additional kwargs to pass to the `apply_chat_template` method.\n",
            " |      Returns:\n",
            " |          `list[int]`: A list of token ids representing the tokenized message.\n",
            " |\n",
            " |  encode_plus(self, text: Union[str, list[str], list[int]], text_pair: Union[str, list[str], list[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
            " |\n",
            " |      <Tip warning={true}>\n",
            " |\n",
            " |      This method is deprecated, `__call__` should be used instead.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          text (`str`, `list[str]` or (for non-fast tokenizers) `list[int]`):\n",
            " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
            " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |          text_pair (`str`, `list[str]` or `list[int]`, *optional*):\n",
            " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
            " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence is provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |\n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |\n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |\n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |\n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |\n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |\n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |\n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |\n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |\n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |\n",
            " |  get_chat_template(self, chat_template: Optional[str] = None, tools: Optional[list[dict]] = None) -> str\n",
            " |      Retrieve the chat template string used for tokenizing chat messages. This template is used\n",
            " |      internally by the `apply_chat_template` method and can also be used externally to retrieve the model's chat\n",
            " |      template for better generation tracking.\n",
            " |\n",
            " |      Args:\n",
            " |          chat_template (`str`, *optional*):\n",
            " |              A Jinja template or the name of a template to use for this conversion.\n",
            " |              It is usually not necessary to pass anything to this argument,\n",
            " |              as the model's template will be used by default.\n",
            " |          tools (`list[Dict]`, *optional*):\n",
            " |              A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
            " |              support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
            " |              giving the name, description and argument types for the tool. See our\n",
            " |              [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n",
            " |              for more information.\n",
            " |\n",
            " |      Returns:\n",
            " |          `str`: The chat template string.\n",
            " |\n",
            " |  get_special_tokens_mask(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False) -> list[int]\n",
            " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
            " |      special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
            " |\n",
            " |      Args:\n",
            " |          token_ids_0 (`list[int]`):\n",
            " |              List of ids of the first sequence.\n",
            " |          token_ids_1 (`list[int]`, *optional*):\n",
            " |              List of ids of the second sequence.\n",
            " |          already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the token list is already formatted with special tokens for the model.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
            " |\n",
            " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, list[transformers.tokenization_utils_base.BatchEncoding], dict[str, list[int]], dict[str, list[list[int]]], list[dict[str, list[int]]]], padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_attention_mask: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
            " |      in the batch.\n",
            " |\n",
            " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
            " |      `self.pad_token_id` and `self.pad_token_type_id`).\n",
            " |\n",
            " |      Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
            " |      text followed by a call to the `pad` method to get a padded encoding.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
            " |      result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
            " |      PyTorch tensors, you will lose the specific device of your tensors however.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `dict[str, list[int]]`, `dict[str, list[list[int]]` or `list[dict[str, list[int]]]`):\n",
            " |              Tokenized inputs. Can represent one input ([`BatchEncoding`] or `dict[str, list[int]]`) or a batch of\n",
            " |              tokenized inputs (list of [`BatchEncoding`], *dict[str, list[list[int]]]* or *list[dict[str,\n",
            " |              list[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
            " |              collate function.\n",
            " |\n",
            " |              Instead of `list[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
            " |              the note above for the return type.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
            " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
            " |               index) among:\n",
            " |\n",
            " |              - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Maximum length of the returned list and optionally padding length (see above).\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value.\n",
            " |\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |\n",
            " |  prepare_for_model(self, ids: list[int], pair_ids: Optional[list[int]] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
            " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
            " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*\n",
            " |      different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return\n",
            " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
            " |\n",
            " |      Args:\n",
            " |          ids (`list[int]`):\n",
            " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
            " |              `convert_tokens_to_ids` methods.\n",
            " |          pair_ids (`list[int]`, *optional*):\n",
            " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
            " |              and `convert_tokens_to_ids` methods.\n",
            " |\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence is provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |\n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          padding_side (`str`, *optional*):\n",
            " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
            " |              Default value is picked from the class attribute of the same name.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |\n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |\n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |\n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |\n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |\n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |\n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |\n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |\n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |\n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |\n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |\n",
            " |  prepare_seq2seq_batch(self, src_texts: list[str], tgt_texts: Optional[list[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: Optional[str] = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
            " |\n",
            " |      Arguments:\n",
            " |          src_texts (`list[str]`):\n",
            " |              List of documents to summarize or source language texts.\n",
            " |          tgt_texts (`list`, *optional*):\n",
            " |              List of summaries or target language texts.\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
            " |              left unset or set to `None`, this will use the predefined model maximum length if a maximum length is\n",
            " |              required by one of the truncation/padding parameters. If the model has no specific maximum input length\n",
            " |              (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          max_target_length (`int`, *optional*):\n",
            " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
            " |              to `None`, this will use the max_length value.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |\n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |\n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          **kwargs:\n",
            " |              Additional keyword arguments passed along to `self.__call__`.\n",
            " |\n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |\n",
            " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
            " |          - **labels** -- List of token ids for tgt_texts.\n",
            " |\n",
            " |          The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.\n",
            " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
            " |\n",
            " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str\n",
            " |      Upload the tokenizer files to the ðŸ¤— Model Hub.\n",
            " |\n",
            " |      Parameters:\n",
            " |          repo_id (`str`):\n",
            " |              The name of the repository you want to push your tokenizer to. It should contain your organization name\n",
            " |              when pushing to a given organization.\n",
            " |          use_temp_dir (`bool`, *optional*):\n",
            " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
            " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Will default to `\"Upload tokenizer\"`.\n",
            " |          private (`bool`, *optional*):\n",
            " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
            " |          token (`bool` or `str`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
            " |              is not specified.\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
            " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
            " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
            " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
            " |              Google Colab instances without any CPU OOM issues.\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
            " |          revision (`str`, *optional*):\n",
            " |              Branch to push the uploaded files to.\n",
            " |          commit_description (`str`, *optional*):\n",
            " |              The description of the commit that will be created\n",
            " |          tags (`list[str]`, *optional*):\n",
            " |              List of tags to push on the Hub.\n",
            " |\n",
            " |      Examples:\n",
            " |\n",
            " |      ```python\n",
            " |      from transformers import AutoTokenizer\n",
            " |\n",
            " |      tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
            " |\n",
            " |      # Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\n",
            " |      tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
            " |\n",
            " |      # Push the tokenizer to an organization with the name \"my-finetuned-bert\".\n",
            " |      tokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
            " |      ```\n",
            " |\n",
            " |  save_chat_templates(self, save_directory: Union[str, os.PathLike], tokenizer_config: dict, filename_prefix: Optional[str], save_jinja_files: bool)\n",
            " |      Writes chat templates out to the save directory if we're using the new format, and removes them from\n",
            " |      the tokenizer config if present. If we're using the legacy format, it doesn't write any files, and instead\n",
            " |      writes the templates to the tokenizer config in the correct format.\n",
            " |\n",
            " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None, push_to_hub: bool = False, **kwargs) -> tuple[str, ...]\n",
            " |      Save the full tokenizer state.\n",
            " |\n",
            " |\n",
            " |      This method make sure the full tokenizer can then be re-loaded using the\n",
            " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..\n",
            " |\n",
            " |      Warning,None This won't save modifications you may have applied to the tokenizer after the instantiation (for\n",
            " |      instance, modifying `tokenizer.do_lower_case` after creation).\n",
            " |\n",
            " |      Args:\n",
            " |          save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
            " |          legacy_format (`bool`, *optional*):\n",
            " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
            " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
            " |              added_tokens files.\n",
            " |\n",
            " |              If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with\n",
            " |              \"slow\" tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be\n",
            " |              loaded in the corresponding \"slow\" tokenizer.\n",
            " |\n",
            " |              If `True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a value\n",
            " |              error is raised.\n",
            " |          filename_prefix (`str`, *optional*):\n",
            " |              A prefix to add to the names of the files saved by the tokenizer.\n",
            " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
            " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
            " |              namespace).\n",
            " |          kwargs (`dict[str, Any]`, *optional*):\n",
            " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |\n",
            " |      Returns:\n",
            " |          A tuple of `str`: The files saved.\n",
            " |\n",
            " |  truncate_sequences(self, ids: list[int], pair_ids: Optional[list[int]] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> tuple[list[int], list[int], list[int]]\n",
            " |      Truncates a sequence pair in-place following the strategy.\n",
            " |\n",
            " |      Args:\n",
            " |          ids (`list[int]`):\n",
            " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
            " |              `convert_tokens_to_ids` methods.\n",
            " |          pair_ids (`list[int]`, *optional*):\n",
            " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
            " |              and `convert_tokens_to_ids` methods.\n",
            " |          num_tokens_to_remove (`int`, *optional*, defaults to 0):\n",
            " |              Number of tokens to remove using the truncation strategy.\n",
            " |          truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `'longest_first'`):\n",
            " |              The strategy to follow for truncation. Can be:\n",
            " |\n",
            " |              - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will truncate\n",
            " |                token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a\n",
            " |                batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n",
            " |                than the model maximum admissible input size).\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
            " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
            " |\n",
            " |      Returns:\n",
            " |          `tuple[list[int], list[int], list[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of\n",
            " |          overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair\n",
            " |          of sequences (or a batch of pairs) is provided.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |\n",
            " |  convert_added_tokens(obj: Union[tokenizers.AddedToken, Any], save=False, add_type_field=True) from builtins.type\n",
            " |\n",
            " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', trust_remote_code=False, **kwargs) from builtins.type\n",
            " |      Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
            " |      tokenizer.\n",
            " |\n",
            " |      Args:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
            " |              Can be either:\n",
            " |\n",
            " |              - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
            " |              - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
            " |                using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
            " |                `./my_model_directory/`.\n",
            " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
            " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
            " |                `./my_model_directory/vocab.txt`.\n",
            " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
            " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
            " |              standard cache should not be used.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
            " |              exist.\n",
            " |          resume_download:\n",
            " |              Deprecated and ignored. All downloads are now resumed by default when possible.\n",
            " |              Will be removed in v5 of Transformers.\n",
            " |          proxies (`dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `hf auth login` (stored in `~/.huggingface`).\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            " |              identifier allowed by git.\n",
            " |          subfolder (`str`, *optional*):\n",
            " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
            " |              facebook/rag-token-base), specify it here.\n",
            " |          inputs (additional positional arguments, *optional*):\n",
            " |              Will be passed along to the Tokenizer `__init__` method.\n",
            " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
            " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
            " |              execute code present on the Hub on your local machine.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
            " |              `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
            " |              `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      Passing `token=True` is required when you want to use a private model.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Examples:\n",
            " |\n",
            " |      ```python\n",
            " |      # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
            " |      # Download vocabulary from huggingface.co and cache.\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |\n",
            " |      # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
            " |\n",
            " |      # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
            " |\n",
            " |      # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
            " |\n",
            " |      # You can link tokens to special vocabulary when instantiating\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", unk_token=\"<unk>\")\n",
            " |      # You should be sure '<unk>' is in the vocabulary when doing that.\n",
            " |      # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
            " |      assert tokenizer.unk_token == \"<unk>\"\n",
            " |      ```\n",
            " |\n",
            " |  register_for_auto_class(auto_class='AutoTokenizer') from builtins.type\n",
            " |      Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n",
            " |      library are already mapped with `AutoTokenizer`.\n",
            " |\n",
            " |\n",
            " |\n",
            " |      Args:\n",
            " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):\n",
            " |              The auto class to register this new tokenizer with.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |\n",
            " |  clean_up_tokenization(out_string: str) -> str\n",
            " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
            " |\n",
            " |      Args:\n",
            " |          out_string (`str`): The text to clean up.\n",
            " |\n",
            " |      Returns:\n",
            " |          `str`: The cleaned-up string.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |\n",
            " |  max_len_sentences_pair\n",
            " |      `int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
            " |\n",
            " |  max_len_single_sentence\n",
            " |      `int`: The maximum length of a sentence that can be fed to the model.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |\n",
            " |  padding_side = 'right'\n",
            " |\n",
            " |  pretrained_vocab_files_map = {}\n",
            " |\n",
            " |  truncation_side = 'right'\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |\n",
            " |  __getattr__(self, key)\n",
            " |\n",
            " |  __setattr__(self, key, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |\n",
            " |  add_special_tokens(self, special_tokens_dict: dict[str, typing.Union[str, tokenizers.AddedToken, collections.abc.Sequence[typing.Union[str, tokenizers.AddedToken]]]], replace_additional_special_tokens=True) -> int\n",
            " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
            " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
            " |      current vocabulary).\n",
            " |\n",
            " |      When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the\n",
            " |      model so that its embedding matrix matches the tokenizer.\n",
            " |\n",
            " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
            " |\n",
            " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
            " |\n",
            " |      - Special tokens can be skipped when decoding using `skip_special_tokens = True`.\n",
            " |      - Special tokens are carefully handled by the tokenizer (they are never split), similar to `AddedTokens`.\n",
            " |      - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
            " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
            " |\n",
            " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
            " |      [`BertTokenizer`] `cls_token` is already registered to be `'[CLS]'` and XLM's one is also registered to be\n",
            " |      `'</s>'`).\n",
            " |\n",
            " |      Args:\n",
            " |          special_tokens_dict (dictionary *str* to *str*, `tokenizers.AddedToken`, or `Sequence[Union[str, AddedToken]]`):\n",
            " |              Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
            " |              `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
            " |\n",
            " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
            " |              assign the index of the `unk_token` to them).\n",
            " |          replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n",
            " |              If `True`, the existing list of additional special tokens will be replaced by the list provided in\n",
            " |              `special_tokens_dict`. Otherwise, `self._special_tokens_map[\"additional_special_tokens\"]` is just extended. In the former\n",
            " |              case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged\n",
            " |              as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the\n",
            " |              `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous\n",
            " |              `additional_special_tokens` are still added tokens, and will not be split by the model.\n",
            " |\n",
            " |      Returns:\n",
            " |          `int`: Number of tokens added to the vocabulary.\n",
            " |\n",
            " |      Examples:\n",
            " |\n",
            " |      ```python\n",
            " |      # Let's see how to add a new classification token to GPT-2\n",
            " |      tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
            " |      model = GPT2Model.from_pretrained(\"openai-community/gpt2\")\n",
            " |\n",
            " |      special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
            " |\n",
            " |      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
            " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
            " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
            " |      model.resize_token_embeddings(len(tokenizer))\n",
            " |\n",
            " |      assert tokenizer.cls_token == \"<CLS>\"\n",
            " |      ```\n",
            " |\n",
            " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, collections.abc.Sequence[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
            " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
            " |      it with indices starting from length of the current vocabulary and will be isolated before the tokenization\n",
            " |      algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
            " |      not treated in the same way.\n",
            " |\n",
            " |      Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
            " |      of the model so that its embedding matrix matches the tokenizer.\n",
            " |\n",
            " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
            " |\n",
            " |      Args:\n",
            " |          new_tokens (`str`, `tokenizers.AddedToken` or a sequence of *str* or `tokenizers.AddedToken`):\n",
            " |              Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
            " |              token to let you personalize its behavior: whether this token should only match against a single word,\n",
            " |              whether this token should strip all potential whitespaces on the left side, whether this token should\n",
            " |              strip all potential whitespaces on the right side, etc.\n",
            " |          special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
            " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
            " |\n",
            " |              See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
            " |\n",
            " |      Returns:\n",
            " |          `int`: Number of tokens added to the vocabulary.\n",
            " |\n",
            " |      Examples:\n",
            " |\n",
            " |      ```python\n",
            " |      # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
            " |      tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |      model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |\n",
            " |      num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
            " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
            " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
            " |      model.resize_token_embeddings(len(tokenizer))\n",
            " |      ```\n",
            " |\n",
            " |  sanitize_special_tokens(self) -> int\n",
            " |      The `sanitize_special_tokens` is now deprecated kept for backward compatibility and will be removed in\n",
            " |      transformers v5.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |\n",
            " |  all_special_ids\n",
            " |      `list[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
            " |\n",
            " |  all_special_tokens\n",
            " |      `list[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).\n",
            " |\n",
            " |      Convert tokens of `tokenizers.AddedToken` type to string.\n",
            " |\n",
            " |  all_special_tokens_extended\n",
            " |      `list[Union[str, tokenizers.AddedToken]]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.), the order has\n",
            " |      nothing to do with the index of each tokens. If you want to know the correct indices, check\n",
            " |      `self.added_tokens_encoder`. We can't create an order anymore as the keys are `AddedTokens` and not `Strings`.\n",
            " |\n",
            " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
            " |      special tokens are tokenized.\n",
            " |\n",
            " |  pad_token_type_id\n",
            " |      `int`: Id of the padding token type in the vocabulary.\n",
            " |\n",
            " |  special_tokens_map\n",
            " |      `dict[str, Union[str, list[str]]]`: A dictionary mapping special token class attributes (`cls_token`,\n",
            " |      `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
            " |\n",
            " |      Convert potential tokens of `tokenizers.AddedToken` type to string.\n",
            " |\n",
            " |  special_tokens_map_extended\n",
            " |      `dict[str, Union[str, tokenizers.AddedToken, list[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping\n",
            " |      special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
            " |\n",
            " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
            " |      special tokens are tokenized.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |\n",
            " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "9fc3866f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a284061f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b1907a3d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.bos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c748564d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [292, 67, 292, 67], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"asdasd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "9aa1103f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_unk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_unk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;31m# TODO: replace tokens not in vocab with '<UNK>'\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_10156/1058154318.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "??replace_unk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74eba64d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_bos_eos(sentence_tokens: list[int]):\n",
        "    return [tokenizer.bos_token_id] + sentence_tokens + [tokenizer.eos_token_id]\n",
        "\n",
        "def tokenize(sentence: str) -> list[int]:\n",
        "    sentence_tokens = tokenizer(sentence)[\"input_ids\"]\n",
        "    sentence_tokens = replace_unk(sentence_tokens, vocab)\n",
        "    sentence_tokens = add_bos_eos(sentence_tokens)\n",
        "\n",
        "    return sentence_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "eabe868b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|endoftext|>', 'Hello', 'Ä world', '.', '<|endoftext|>']"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenize(\"Hello world.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "1b3574ea",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([50256, 15496, 995, 13, 50256], '<|endoftext|>Hello world.<|endoftext|>')"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize(\"Hello world.\"), tokenizer.decode(tokenize(\"Hello world.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa4fd56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of str\n",
        "train_text = extract_texts(train)\n",
        "\n",
        "# List of sequences (i.e. list[int]) of tokens: list[list[int]]\n",
        "train_tokens = []\n",
        "\n",
        "for sentence in train_text:\n",
        "    tokenized_sentence = tokenize(sentence) # list[int]\n",
        "    train_tokens.append(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7363de9",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_tokens = [\n",
        "    tokenize(sentence)\n",
        "    for sentence in train_text\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03c08cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "unigram_counts = Counter()\n",
        "bigram_counts = Counter()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: build train_tokens and dev_tokens using replace_unk\n",
        "# Hint: train_texts = [ex[text_field] for ex in train if ex[text_field].strip()]\n",
        "# Hint: train_tokens = [replace_unk(tokenize_whitespace(t), vocab) for t in train_texts]\n",
        "# Hint: add <BOS> and <EOS> for n-gram counts\n",
        "\n",
        "# TODO: compute unigram_counts and bigram_counts with Counter\n",
        "# Hint: bigrams can be pairs from zip(seq[:-1], seq[1:])\n",
        "\n",
        "# TODO: define bigram_prob(prev, tok, alpha) with add-alpha smoothing\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cadd0ca5",
      "metadata": {},
      "source": [
        "## Step 6: Perplexity and simple grid search\n",
        "We evaluate on the dev split and tune the smoothing strength by trying a few alphas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140081e4",
      "metadata": {},
      "source": [
        "**About perplexity**\n",
        "- Perplexity is the exponent of the average negative log probability.\n",
        "- The minus sign turns log-likelihoods (which are negative) into a positive surprisal value.\n",
        "- The `-1/N` factor averages per token so results are comparable across lengths.\n",
        "- With log, the formula is:\n",
        "$$\\mathrm{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^N \\log p(w_i \\mid w_{i-1})\\right)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6736f9dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement perplexity for a list of token sequences\n",
        "# Hint: use log probabilities and average over total bigrams\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03c1a16",
      "metadata": {},
      "source": [
        "**Why grid search?**\n",
        "- We do not know the best alpha in advance.\n",
        "- A small grid search tries a few candidate values and picks the one\n",
        "  with the lowest dev perplexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e7fa04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: grid search over a few alpha values (e.g., [0.1, 0.5, 1.0])\n",
        "# TODO: print dev perplexity for each and pick the best\n",
        "\n",
        "# Write your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Top-k sampling\n",
        "We generate short samples using a bigram model and top-k sampling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About top-k sampling**\n",
        "- We keep only the top-k most likely next tokens.\n",
        "- Sampling from this trimmed distribution balances variety and coherence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement sample_next(prev, alpha, k) using bigram_prob\n",
        "# Hint: compute probabilities for all tokens, take top-k, sample with np.random.choice\n",
        "\n",
        "# TODO: implement generate(max_len, alpha, k) starting from <BOS>\n",
        "# TODO: print 2-3 generated samples\n",
        "\n",
        "# Write your code below\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
